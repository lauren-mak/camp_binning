'''Workflow for the CAMP binning module.'''


from contextlib import redirect_stderr
import os
from os.path import basename, join
import pandas as pd
from utils import Workflow_Dirs, ingest_samples, cut_up_fasta, make_concoct_table, split_concoct_output


# Load and/or make the working directory structure
dirs = Workflow_Dirs(config['work_dir'], 'binning')


# Load sample names and input files 
SAMPLES = ingest_samples(config['samples'], dirs.TMP)


# --- Workflow output --- #


rule all:
    input:
        join(dirs.OUT, 'final_reports', 'samples.csv') # sample name, bin directories


# --- Workflow steps --- #

rule map_sort:
    input:
        fwd = join(dirs.TMP, '{sample}_1.fastq'),
        rev = join(dirs.TMP, '{sample}_2.fastq'),
        ctg = join(dirs.TMP, '{sample}.fasta'),
    output:
        join(dirs.OUT, '0_contig_coverage', '{sample}', 'coverage.bam'), 
        join(dirs.OUT, '0_contig_coverage', '{sample}', 'coverage.bam.bai'),
    log:
        join(dirs.LOG, 'map_sort', '{sample}.out'),
    threads: config['map_sort_threads'],
    resources:
        mem_mb = lambda wildcards, attempt: \
              int(config['map_sort_mem_mb']) + 10000 * attempt,
    params:
        out_dir = join(dirs.OUT, '0_contig_coverage', '{sample}'),
    shell:
        """
        CTG_PREFIX=$(basename {input.ctg} .fasta)
        mkdir -p {params.out_dir}
        bowtie2-build {input.ctg} {params.out_dir}/$CTG_PREFIX > {log} 2>&1
        bowtie2 -x {params.out_dir}/$CTG_PREFIX -p {threads} \
            -1 {input.fwd} -2 {input.rev} | \
            samtools view -@ {threads} -uS - | \
            samtools sort -@ {threads} - \
            -o {params.out_dir}/coverage.bam > {log} 2>&1
        samtools index -@ {threads} {params.out_dir}/coverage.bam > {log} 2>&1
        """


rule metabat2_calculate_depth:
    input:
        join(dirs.OUT, '0_contig_coverage', '{sample}', 'coverage.bam'),
    output:
        join(dirs.OUT, '1_metabat2', '{sample}', 'coverage.txt'),
    log:
        join(dirs.LOG, 'calculate_depth', 'metabat2_{sample}.out'),
    params:
        out_dir = join(dirs.OUT, '1_metabat2', '{sample}'),
    shell:
        """
        mkdir -p {params.out_dir}
        jgi_summarize_bam_contig_depths {input} --outputDepth {output} \
            > {log} 2>&1
        """


rule metabat2_binning:
    input:
        cov = join(dirs.OUT, '1_metabat2', '{sample}', 'coverage.txt'),
        ctg = join(dirs.TMP, '{sample}.fasta'),
    output:
        join(dirs.OUT, '1_metabat2', '{sample}_done.txt'),
    log:
        join(dirs.LOG, 'metabat2_binning', '{sample}.out'), 
    threads: config['metabat2_binning_threads'],
    resources:
        mem_mb = lambda wildcards, attempt: \
              int(config['metabat2_binning_mem_mb']) + 40000 * attempt,
    params:
        min_len = config['min_contig_len'],
        out_dir = join(dirs.OUT, '1_metabat2', '{sample}'),
    shell:
        """
        metabat2 -m {params.min_len} -t {threads} --unbinned \
            -i {input.ctg} -a {input.cov} -o {params.out_dir}/bin > {log} 2>&1
        mv {params.out_dir}/bin.unbinned.fa {params.out_dir}/unbinned.fa
        touch {output}
        """


rule concoct_calculate_depth:
    input:
        ctg = join(dirs.TMP, '{sample}.fasta'),
        bam = join(dirs.OUT, '0_contig_coverage', '{sample}', 'coverage.bam'), 
    output:
        fa = join(dirs.OUT, '2_concoct', '{sample}', \
            str(config['fragment_size']) + '.fasta'),
        cov = join(dirs.OUT, '2_concoct', '{sample}', 'coverage.txt'),
    log:
        join(dirs.LOG, 'calculate_depth', 'concoct_{sample}.out'),
    params:
        frag_size = config['fragment_size'],
        olap_size = config['overlap_size'],
        out_dir = join(dirs.OUT, '2_concoct', '{sample}'),
    run:
        with open(log[0], 'w') as l:
            with redirect_stderr(l):
                print('Now writing to {}'.format(log))
                outbed = "{}/{}.bed".format(params.out_dir, params.frag_size)
                cut_up_fasta(input.ctg, params.frag_size, params.olap_size, \
                    params.out_dir, output.fa, outbed)
                make_concoct_table(outbed, input.bam, output.cov)


rule concoct_binning:
    input:
        fa = join(dirs.OUT, '2_concoct', '{sample}', \
            str(config['fragment_size']) + '.fasta'),
        cov = join(dirs.OUT, '2_concoct', '{sample}', 'coverage.txt'),
    output:
        join(dirs.OUT, '2_concoct', '{sample}', \
             'clustering_gt' + str(config['min_contig_len']) + '.csv'),
    conda:
        join(config['env_yamls'], 'concoct.yaml'),
    log:
        join(dirs.LOG, 'concoct_binning', '{sample}.out'), 
    resources:
        mem_mb = lambda wildcards, attempt: \
              int(config['concoct_binning_mem_mb']) + 40000 * attempt,
    params:
        min_len = config['min_contig_len'],
        out_dir = join(dirs.OUT, '2_concoct', '{sample}'),
    shell:
        """
        mkdir -p {params.out_dir}
        concoct --composition_file {input.fa} --coverage_file {input.cov} \
            -l {params.min_len} -b {params.out_dir}/ > {log} 2>&1
        sed -i '1i contig_id,cluster_id' {output}
        """


rule split_concoct_output:
    input:
        concoct = join(dirs.OUT, '2_concoct', '{sample}', \
             'clustering_gt' + str(config['min_contig_len']) + '.csv'),
        ctg = join(dirs.TMP, '{sample}.fasta'),
    output:
        join(dirs.OUT, '2_concoct', '{sample}_done.txt'),
    params:
        out_dir = join(dirs.OUT, '2_concoct', '{sample}'),
    run:
        split_concoct_output(input.concoct, input.ctg, params.out_dir, output)
        open(str(output), 'a').close()


rule vamb_binning:
    input:
        ctg = join(dirs.TMP, '{sample}.fasta'), 
        bam = join(dirs.OUT, '0_contig_coverage', '{sample}', 'coverage.bam'),
    output:
        join(dirs.OUT, '3_vamb', '{sample}', 'clusters.tsv'),
    log:
        join(dirs.LOG, 'vamb_binning', '{sample}.out'),
    conda:
        join(config['env_yamls'], 'vamb.yaml'),
    threads: config['vamb_binning_threads'],
    resources:
        mem_mb = lambda wildcards, attempt: \
              int(config['vamb_binning_mem_mb']) + 40000 * attempt,
    params:
        out_dir = join(dirs.OUT, '3_vamb', '{sample}'),
        min_len = config['min_contig_len'],
        min_bin = config['min_bin_size'],
    shell:
        """
        vamb --outdir {params.out_dir} \
            --fasta {input.ctg} \
            --bamfiles {input.bam} \
            -m {params.min_len} \
            --minfasta {params.min_bin} -o C \
            > {log} 2>&1 
        """


rule split_vamb_output:
    input:
        vamb = join(dirs.OUT, '3_vamb', '{sample}', 'clusters.tsv'),
        ctg = join(dirs.TMP, '{sample}.fasta'),
    output:
        join(dirs.OUT, '3_vamb', '{sample}_done.txt'),
    conda:
        join(config['env_yamls'], 'vamb.yaml'),
    params:
        out_dir = join(dirs.OUT, '3_vamb', '{sample}'),
    run:
        from vamb.vambtools import Reader, read_clusters, loadfasta, write_bins
        bins = read_clusters(str(input.vamb))
        with Reader(str(input.ctg), 'rb') as f:
            fasta = loadfasta(f)
        write_bins(str(params.out_dir), bins, fasta)
        open(str(output), 'a').close()


rule make_config:
    input:
        expand(join(dirs.OUT, '{binner}', '{sample}_done.txt'), \
                      binner = BINNERS, sample = SAMPLES),
    output:
        join(dirs.OUT, 'final_reports', 'samples.csv'),
    run:
        dct = {}
        for i in input:
            info = str(i).split('/')
            s = info[-1].split('_')[0]
            d = info[-2].split('_')[1]
            if s not in dct:
                dct[s] = {}
            dct[s][d] = join(*info[:-1] + [s])
        df = pd.DataFrame.from_dict(dct, orient ='index')
        df.reset_index(inplace = True)
        df.rename(columns = {'index': 'sample_name'}, inplace = True)
        df.to_csv(str(output), index = False)



